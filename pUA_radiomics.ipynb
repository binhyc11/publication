{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da71b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils, os, pandas, glob, logging, random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import SimpleITK as sitk\n",
    "# from radiomics import featureextractor\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d06131",
   "metadata": {},
   "source": [
    "# Define basic functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e66e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dcm_file(dcm_dir):\n",
    "    \"\"\"\n",
    "    Read CT images into a SimpleITK object.\n",
    "    Parameter:\n",
    "        A directory of CT images.\n",
    "    Returns:\n",
    "        SimpleITK object of the images.\n",
    "    \"\"\"\n",
    "    dcm_files = glob.glob(os.path.join(dcm_dir, '*.dcm'))\n",
    "    dcm_files.sort(reverse=True)\n",
    "    dcm_image = sitk.ReadImage(dcm_files)\n",
    "    return dcm_image\n",
    "\n",
    "def read_mask_file(ROI_dir):\n",
    "    \"\"\"\n",
    "    Read segmentation mask into a SimpleITK object.\n",
    "    Parameter: \n",
    "        A directory of the mask.\n",
    "    Returns: \n",
    "        SimpleITK object of the mask.\n",
    "    \"\"\"\n",
    "    return sitk.ReadImage(ROI_dir)\n",
    "\n",
    "def extract_one_sample (dcm_dir, ROI_dir, params):\n",
    "    \"\"\"\n",
    "    Extract radiomics features for each pair of CT scans and the corresponding mask.\n",
    "    Parameters: \n",
    "        dcm_dir: directory of CT images.\n",
    "        ROI_dir: directory of the mask.\n",
    "        params: directory of the setting parameters file required for the extraction.\n",
    "    Returns: \n",
    "       radiomics: a dictionary of names and values of radiomics features.\n",
    "    \"\"\"\n",
    "    extractor = featureextractor.RadiomicsFeatureExtractor(params)\n",
    "    dcm_img = read_dcm_file(dcm_dir)\n",
    "    ROI = read_mask_file(ROI_dir)\n",
    "    radiomics = extractor.execute(dcm_img, ROI)\n",
    "    return radiomics\n",
    "\n",
    "def get_features_name():\n",
    "    \"\"\"\n",
    "    Get a list of names of radiomics features.\n",
    "    \"\"\"\n",
    "    data2 = utils.load_json(r\"E:\\UA_radiomics\\data_json\\11_train_sample.json\")\n",
    "    return list(data2['71057994_CT_ax']['radiomics'].keys())\n",
    "\n",
    "def get_features_index():\n",
    "    \"\"\"\n",
    "    Set an index to each feature.\n",
    "    Returns: \n",
    "        features_index: a dictionary of names and indices of radiomics features.\n",
    "    \"\"\"\n",
    "    features_name = get_features_name()\n",
    "    features_index = {}\n",
    "    for ID in range (len(features_name)):\n",
    "        features_index[features_name[ID]] = ID\n",
    "    return features_index\n",
    "\n",
    "def get_radiomics_label(IDs, data):\n",
    "    \"\"\"\n",
    "    Get radiomics feature values and label of each sample.\n",
    "    Parameters:\n",
    "        IDs: list of sample IDs.\n",
    "        data: dictionary with keys are IDs, and values are information related to those IDs.\n",
    "    Returns:\n",
    "        features: a numpy array of radiomics feature values of all the samples in IDs.\n",
    "        label: a list of samples' label.\n",
    "    \"\"\"\n",
    "    features, label = [],[]\n",
    "    for ID in IDs:\n",
    "        label.append(data[ID]['label'])\n",
    "        features.append(list(data[ID]['radiomics'].values()))\n",
    "    features = np.asanyarray(features)\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c3e6b8",
   "metadata": {},
   "source": [
    "# Define 2 functions for ruling out suspected sampling error samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2d2bf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_HU (dcm_dir, ROI):\n",
    "    \"\"\"\n",
    "    Determine the maximum HU value inside the mask.\n",
    "    Parameters:\n",
    "        dcm_dir: directory of CT images.\n",
    "        ROI: SimpleITK object of the mask.\n",
    "    Returns:\n",
    "        maximum_HU: float, the maximum HU value inside the mask.\n",
    "    \"\"\"\n",
    "    dcm = read_dcm_file(dcm_dir)\n",
    "    dcm = sitk.GetArrayFromImage(dcm)\n",
    "    stone = np.multiply(dcm, ROI)\n",
    "    maximum_HU = 0\n",
    "    for i in range (len(stone)):\n",
    "        if stone[i].max() > maximum_HU:\n",
    "            maximum_HU = stone[i].max()\n",
    "    return float(maximum_HU)\n",
    "\n",
    "def exclusion(dataset):\n",
    "    \"\"\"\n",
    "    To remove samples whose suspected maxmium HU value (i.e., labeled as pure uric acid but have maxmium HU > 1300).\n",
    "    Parameter:\n",
    "        dataset: dictionary contains all pre-screened samples.\n",
    "    Returns:\n",
    "        data: dictionary only contains samples that met selection criterion.\n",
    "    \"\"\"\n",
    "    exclude = []\n",
    "    for k, v in dataset.items():\n",
    "        for k1, v1 in v.items():\n",
    "            if modality in k1:\n",
    "                sample_ID = k1 \n",
    "                dcm_dir = v1['dcm_dir']\n",
    "                ROI_dir = v1['ROI_dir']\n",
    "                ROI = read_mask_file(ROI_dir)\n",
    "                ROI = sitk.GetArrayFromImage(ROI)[:, :, :, 0]\n",
    "                if v['label'] == 1:\n",
    "                    if max_HU (dcm_dir, ROI) > 1300:\n",
    "                        exclude.append(k)\n",
    "    data = {}\n",
    "    for k, v in dataset.items():\n",
    "        if k not in exclude:\n",
    "            data[k] = v\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aa0cc8",
   "metadata": {},
   "source": [
    "# Define a function for creating the working data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd1dfcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_dict ():\n",
    "    \"\"\"\n",
    "    Create a dictionary with keys are instance IDs, values include label, directory of CT images and the corresponding mask.\n",
    "    \"\"\" \n",
    "    #### For 2019 & 2020 cohort ####\n",
    "    files_19_20 = os.listdir(ROI_path_19_20)\n",
    "    IDs, data = [], {}\n",
    "    for file in files_19_20:\n",
    "        if modality in file:\n",
    "            filename = file.split('.')[0]\n",
    "            ID = filename.split('_')[0]\n",
    "            if '(' in filename:\n",
    "                dcm_folder = filename [len(ID)+1:-3]\n",
    "            else:\n",
    "                dcm_folder = filename [len(ID)+1:]\n",
    "            dcm_dir = dcm_path_19_20 + ID + '/' + dcm_folder +'/'\n",
    "            ROI_dir = ROI_path_19_20 + file\n",
    "            \n",
    "            if ID not in IDs:\n",
    "                IDs.append(ID)\n",
    "                data[ID] = {'label': 1, filename: {'dcm_dir': dcm_dir, 'ROI_dir': ROI_dir}}\n",
    "            else:\n",
    "                data[ID][filename] = {'dcm_dir': dcm_dir, 'ROI_dir': ROI_dir}\n",
    "    #### For 2021 cohort ####\n",
    "    files_21 = os.listdir(ROI_path_21)\n",
    "    ROI_file_names = []\n",
    "    for file in files_21:\n",
    "        if modality in file:\n",
    "            ROI_file_names.append(file.split('.')[0])\n",
    "            \n",
    "    df = pandas.read_csv(label_file, dtype=str)\n",
    "    for _, row in df.iterrows():\n",
    "        patient_id = row['ID']\n",
    "        if len (patient_id) == 7:\n",
    "            patient_id = '0' + patient_id\n",
    "        label = int(row['label'])\n",
    "        if label == 0:\n",
    "            label = -1\n",
    "        data[patient_id] = {'label': label}\n",
    "        for ROI_file_name in ROI_file_names:\n",
    "            if ROI_file_name.startswith(patient_id):\n",
    "                data[patient_id][ROI_file_name] = {'label': label}\n",
    "\n",
    "    for i in files_21:\n",
    "        if modality in i:\n",
    "            filename = i.split('.')[0]\n",
    "            ID = filename.split('_')[0]\n",
    "            if '(' in filename:\n",
    "                dcm_folder = filename[len(ID)+1:-3]\n",
    "            else:\n",
    "                dcm_folder = filename [len(ID)+1:]\n",
    "            dcm_dir = dcm_path_21 + ID + '/' + dcm_folder +'/'\n",
    "            ROI_dir = ROI_path_21 + i\n",
    "            data[ID][filename]['dcm_dir'] = dcm_dir\n",
    "            data[ID][filename]['ROI_dir'] = ROI_dir\n",
    "    #### remove keys with no samples ####\n",
    "    data2 = {}\n",
    "    for k, v in data.items():\n",
    "        if len(v) > 1:\n",
    "            data2[k] = v\n",
    "    return data2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c3194",
   "metadata": {},
   "source": [
    "# Define a function for creating splitting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c525c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets (seeding, data):  ### dataset's keys are instance_IDs\n",
    "    \"\"\"\n",
    "    Randomly split data in to test and train data sets. For train set, further split into 5 folds.\n",
    "    Parameters:\n",
    "        seeding: int, for reproducibly random split.\n",
    "        data: dictionary with keys are instances' ID.\n",
    "    Returns:\n",
    "        training_sample: dictionary, train dataset with keys are samples' IDs. For each sample ID, there is info indicating whether sub_train or validation sets it belongs to in each fold \n",
    "                        (e.g., in Fold_1, it belongs to validation set, to sub_train sets in all the others).\n",
    "        testing_sample: dictionary, test dataset with keys are samples' IDs\n",
    "    \"\"\"\n",
    "    other, UA = {}, {}\n",
    "    for k, v in data.items():\n",
    "        if v['label'] == -1:\n",
    "            other[k] = v\n",
    "        if v['label'] == 1:\n",
    "            UA[k] = v\n",
    "\n",
    "    UA_IDs    = list (UA.keys())\n",
    "    other_IDs = list (other.keys())\n",
    "\n",
    "    random.seed(seeding)\n",
    "    train_UA_IDs = random.sample(UA_IDs, k = 95)\n",
    "    test_UA_IDs  = list (set(UA_IDs) - set(train_UA_IDs))\n",
    "\n",
    "    random.seed(seeding)\n",
    "    train_others_IDs = random.sample(other_IDs, k = 120)\n",
    "    test_others_IDs  = list (set(other_IDs) - set(train_others_IDs))\n",
    "\n",
    "    training_pt, testing_pt = {}, {} # for instances\n",
    "    for k, v in data.items():\n",
    "        if k in train_UA_IDs:\n",
    "            training_pt[k] = v\n",
    "        if k in train_others_IDs:\n",
    "            training_pt[k] = v\n",
    "        if k in test_UA_IDs:\n",
    "            testing_pt[k] = v\n",
    "        if k in test_others_IDs:\n",
    "            testing_pt[k] = v\n",
    "    \n",
    "    training_sample, testing_sample = {}, {} # for samples\n",
    "    for _, v1 in training_pt.items():\n",
    "        for i in v1.keys():\n",
    "            if 'ax' in i:\n",
    "                training_sample[i] = v1[i]\n",
    "                training_sample[i]['label'] = v1['label']\n",
    "    for _, v2 in testing_pt.items():\n",
    "        for j in v2.keys():\n",
    "            if 'ax' in j:\n",
    "                testing_sample[j] = v2[j]\n",
    "                testing_sample[j]['label'] = v2['label']\n",
    "    \n",
    "    pure_UA = np.array([k for k, v in training_sample.items() if v['label'] ==  1])\n",
    "    others  = np.array([k for k, v in training_sample.items() if v['label'] == -1])\n",
    "\n",
    "    kf =  KFold(n_splits = number_of_folds, random_state = seeding, shuffle = True)\n",
    "\n",
    "    pure_UA_train, pure_UA_val = [], []\n",
    "    for train_index, val_index in kf.split(pure_UA):\n",
    "        pure_UA_train.append(list(pure_UA[train_index]))\n",
    "        pure_UA_val  .append(list(pure_UA[val_index  ]))\n",
    "\n",
    "    others_train,  others_val  = [], []\n",
    "    for train_index, val_index in kf.split(others):\n",
    "        others_train.append(list(others[train_index]))\n",
    "        others_val  .append(list(others[val_index  ]))\n",
    "        \n",
    "    for i in range (number_of_folds):\n",
    "        for j in pure_UA_train[i]:\n",
    "            training_sample[j]['fold_' + str(i)] = 'train'\n",
    "        for k in others_train[i]:\n",
    "            training_sample[k]['fold_' + str(i)] = 'train'\n",
    "\n",
    "        for l in pure_UA_val[i]:\n",
    "            training_sample[l]['fold_' + str(i)] = 'val'\n",
    "        for m in others_val[i]:\n",
    "            training_sample[m]['fold_' + str(i)] = 'val'\n",
    "    return training_sample, testing_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c01dd30",
   "metadata": {},
   "source": [
    "# Define functions for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87973342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_id_from_fold (fold, data):\n",
    "    \"\"\"\n",
    "    Get sample IDs in train or validation subsets in a particular fold.\n",
    "    Parameters:\n",
    "        fold: int, 0-indexing.\n",
    "        data: dictionary with keys are sample IDs.\n",
    "    Returns:\n",
    "        train_ids: list, sample IDs in train subset.\n",
    "        val_ids: list, sample IDs in validation subset.\n",
    "    \"\"\"\n",
    "    train_ids, val_ids =[], []\n",
    "    for sample_id, v in data.items():\n",
    "        if v['fold_'+ str(fold)] == 'train':\n",
    "            train_ids.append(sample_id)\n",
    "        if v['fold_'+ str(fold)] == 'val':\n",
    "            val_ids.append(sample_id)\n",
    "    return train_ids, val_ids\n",
    "\n",
    "def train_val_split (ids):\n",
    "    \"\"\"\n",
    "    Randomly divide a set of IDs into 2 parts with the ratio of 8:2.\n",
    "    Parameter:\n",
    "        ids: list of IDs.\n",
    "    Returns:\n",
    "        train_ids: list of 80% of IDs.\n",
    "        val_ids: list of 20% of IDs.\n",
    "    \"\"\"\n",
    "    val_size = int(len(ids) * 0.2)\n",
    "    val_ids = np.random.choice (ids, val_size, replace=False).tolist()\n",
    "    train_ids = list (set(ids) - set(val_ids))\n",
    "    return train_ids, val_ids\n",
    "\n",
    "def select_features (alpha_value, train_data, train_label, val_data, val_label):\n",
    "    \"\"\"\n",
    "    Get features' index and coefficient from LASSO. Only features with non-zero coefficients are filtered.\n",
    "    Parameters:\n",
    "        alpha_value: float, regularization value for LASSO.\n",
    "        train_data: numpy array of radiomics feature values for fitting LASSO.\n",
    "        train_label: list of labels of train set.\n",
    "        val_data: numpy array of radiomics feature values for evaluating LASSO.\n",
    "        val_label: list of labels of validation set.\n",
    "    Returns:\n",
    "        ft_idx: numpy array of non-zero features' index \n",
    "        ft_coef: numpy array of coefficients.\n",
    "    \"\"\"\n",
    "    lasso = linear_model.Lasso(alpha = alpha_value)\n",
    "    lasso.fit(train_data, train_label)\n",
    "    ft_idx  = np.where(lasso.coef_ != 0)[0]\n",
    "    ft_coef = lasso.coef_[ft_idx]\n",
    "    return ft_idx, ft_coef\n",
    "\n",
    "def features_freq_coef (alpha_value, train_ids, data):\n",
    "    \"\"\"\n",
    "    Get accumulated selection times and summation of absolute LASSO coefficient.\n",
    "    Parameters:\n",
    "        alpha_value: float, regularization value for LASSO.\n",
    "        train_ids: list of sample IDs.\n",
    "    Returns:\n",
    "        freq: list of feature indices in descending order of accumulated selection times.\n",
    "        coef: list of feature indices in descending order of summation of absolute coefficients.\n",
    "    \"\"\"    \n",
    "    frequency, coeficient = [], []\n",
    "    features_name         = get_features_name()\n",
    "    for ft in features_name:\n",
    "        frequency .append([ft, 0])\n",
    "        coeficient.append([ft, 0])\n",
    "    \n",
    "    for i in range(100):\n",
    "        train_subset_ids, val_ids = train_val_split(train_ids)\n",
    "        \n",
    "        train_data2, train_label = get_radiomics_label (train_subset_ids, data)\n",
    "        val_data2  , val_label   = get_radiomics_label (val_ids         , data)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_data2)\n",
    "        \n",
    "        train_data = scaler.transform (train_data2)\n",
    "        val_data   = scaler.transform (val_data2)\n",
    "        \n",
    "        ft_idx, ft_coef = select_features (alpha_value, train_data, train_label, val_data, val_label)\n",
    "        for idx in range (len(ft_idx)):\n",
    "            frequency  [ft_idx[idx]][1] += 1\n",
    "            coeficient [ft_idx[idx]][1] += float(abs(ft_coef[idx]))\n",
    "        \n",
    "    freq = sorted(frequency,  key = lambda item: item[1], reverse = True)\n",
    "    coef = sorted(coeficient, key = lambda item: item[1], reverse = True)\n",
    "    return freq, coef\n",
    "\n",
    "def most_crucial_fts (number_of_folds, number_features, data, modality, alpha):\n",
    "    \"\"\"\n",
    "    Select the most important features based on their coefficients and selected times.\n",
    "    Parameters:\n",
    "        number_of_folds: int, number of folds for cross-validation.\n",
    "        number_features: int, number of features to be selected.\n",
    "        data: dictionary, with keys are IDs, and values are information related to those IDs.\n",
    "        modality: str, the acquisition plane of interest (e.g., CT_ax: axial plane).\n",
    "        alpha: float, regularization value for LASSO.\n",
    "    Returns: \n",
    "        A list of selected features' indices.\n",
    "        Note that number of features in this list might differ from number_features due to some unmet selection criteria.\n",
    "    \"\"\"\n",
    "    features = get_features_index()\n",
    "    for k, v in features.items():\n",
    "        features[k] = {'index': v, 'freq': 0, 'coef': 0}\n",
    "    for fold in range (number_of_folds):\n",
    "        train_ids, _ = get_train_val_id_from_fold (fold, data)        \n",
    "        freq_fold, coef_fold = features_freq_coef (alpha, train_ids, data)\n",
    "        for i in range (len(freq_fold)):\n",
    "            features[freq_fold[i][0]]['freq'] += freq_fold[i][1]\n",
    "            features[coef_fold[i][0]]['coef'] += coef_fold[i][1]\n",
    "    df = []\n",
    "    for k, v in features.items():\n",
    "        name = k\n",
    "        index = v['index']\n",
    "        freq  = v['freq']\n",
    "        coef  = v['coef']\n",
    "        df.append ([name, index, freq, coef])\n",
    "    freq = sorted(df, key = lambda item: item[2], reverse = True)\n",
    "    coef = sorted(df, key = lambda item: item[3], reverse = True)\n",
    "    selected = []\n",
    "    for i in coef[:20]: # selection criteria prioritizes coef\n",
    "        if i in freq[:20]:\n",
    "            selected.append (i[1])           \n",
    "    return selected[:number_features]\n",
    "\n",
    "def optimal_set_of_features (crucial_features_idx):\n",
    "    results = []\n",
    "    for n_ft in range (len(crucial_features_idx)):\n",
    "        features_index = crucial_features_idx[:n_ft+1]\n",
    "        acc, sen, spe, AUC = 0, 0, 0, 0\n",
    "        for fold in range (number_of_folds):\n",
    "            _, val_acc, _, sensi, speci, aurocc, _ = training (fold, test_data, test_label, features_index, report_on='val')\n",
    "            acc   += round (val_acc,2)\n",
    "            sen   += round (sensi,2)\n",
    "            spe   += round (speci,2)\n",
    "            AUC   += round (aurocc,2)\n",
    "        results.append([features_index, acc/5, sen/5, spe/5, AUC/5])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429447e3",
   "metadata": {},
   "source": [
    "# Define functions for reporting performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b3103b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(labels, data, classifier, ft_idx, report_test=False):\n",
    "    \"\"\"\n",
    "    Report performance metrics for a particular data set.\n",
    "    Parameters:\n",
    "        labels: list of sample labels.\n",
    "        data: numpy array of radiomics features values.\n",
    "        classifier: the model that has been trained.\n",
    "        ft_idx: list of feature indices in consideration.\n",
    "    Returns:\n",
    "        sensi: float, sensitivity where pure uric acid samples are considered as positive case.\n",
    "        speci: float, specificity.\n",
    "        auc: float, area under the ROC curve.\n",
    "    \"\"\"\n",
    "    title = list(map(str, ft_idx))\n",
    "    title = ''.join(str(ft_idx))\n",
    "    preds = classifier.predict(data)\n",
    "    probs = classifier.predict_proba(data)\n",
    "    trans_labels = []\n",
    "    for label in labels:\n",
    "        if label == -1:\n",
    "            trans_labels.append(0)\n",
    "        else:\n",
    "            trans_labels.append(1)\n",
    "    for index in range(len(preds)):\n",
    "        if preds[index] == -1:\n",
    "            preds[index] = 0\n",
    "\n",
    "    acc = utils.cal_simple_acc(preds, trans_labels)\n",
    "    sensi, speci = utils.cal_sensi_speci(trans_labels, preds)\n",
    "#     conf_matrix  = confusion_matrix(trans_labels, preds)\n",
    "#     utils.make_confusion_matrix (conf_matrix, categories= ['Others', 'Pure uric acid'], title=title)\n",
    "    \n",
    "    if report_test:\n",
    "        probs = np.array(probs)\n",
    "        trans_labels = np.array(trans_labels)\n",
    "        f1_score, auc = utils.Precision_Recall_curve(preds, trans_labels, probs[:, 1])\n",
    "#         print (f'Sen: {round(sensi,3)}; Spe: {round(speci,3)}; AUPRC: {round(auc,3)}; F1: {round(f1_score, 3)}')\n",
    "#         print ('_________________________')\n",
    "    else: \n",
    "        f1_score = 0\n",
    "        auc = utils.cal_auc(trans_labels, probs, title)\n",
    "#         utils.visualize_roc (trans_labels, probs, title)\n",
    "#         print (f'Sen: {round(sensi,3)}; Spe: {round(speci,3)}; AUC: {round(auc,3)}')\n",
    "#         print ('_________________________')\n",
    "    return sensi, speci, auc, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b638f6",
   "metadata": {},
   "source": [
    "# Define a function for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dab1b2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    " def training(fold, test_data, test_label, features_selected_index, report_on='test'):\n",
    "    \"\"\"\n",
    "    Train the model with optimal hyperparameters and selected features.\n",
    "    Report performance for train, validation or test set by uncommment on corresponding line of code.\n",
    "    Parameters:\n",
    "        fold: int, 0-indexing.\n",
    "        test_data: numpy array of radiomics features values of test set.\n",
    "        test_label: list of sample labels of test set\n",
    "        report: str, specify which set reporting performance on. 'test' by default.\n",
    "    Returns:\n",
    "        train_acc: float, accuracy on train set.\n",
    "        val_acc: float, accuracy on validation set.\n",
    "        test_acc: float, accuracy on test set.\n",
    "        sensi: float, sensitivity on the specified set.\n",
    "        speci: float, specificity on the specified set.\n",
    "        auc: float, area under the ROC curve for train or validation set, under the Precicion-Recall curve for test set.\n",
    "        f1_score: float, 0 for train or validation set.\n",
    "    \"\"\"\n",
    "    train_ids, val_ids = get_train_val_id_from_fold (fold, train_dataset)   \n",
    "    train_data, train_label =  get_radiomics_label(train_ids, train_dataset)\n",
    "    val_data  , val_label   =  get_radiomics_label(val_ids  , train_dataset)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_data)\n",
    "    train_data = scaler.transform(train_data)\n",
    "    val_data   = scaler.transform(val_data)\n",
    "    test_data  = scaler.transform(test_data)\n",
    "    \n",
    "    train_data = train_data[:, features_selected_index]\n",
    "    val_data   = val_data  [:, features_selected_index]\n",
    "    test_data  = test_data [:, features_selected_index]\n",
    "    \n",
    "    classifier = SVC(probability=True, random_state=seed, C=C, gamma=gamma)\n",
    "    classifier.fit(train_data, train_label)\n",
    "    \n",
    "    train_acc = classifier.score(train_data, train_label)\n",
    "    val_acc   = classifier.score(val_data  , val_label)\n",
    "    test_acc  = classifier.score(test_data , test_label)\n",
    "\n",
    "#     print (f'Of fold #{fold}')\n",
    "#     print (f\"Train_acc: {round(train_acc,3)}; Val_acc: {round(val_acc,3)}; Test_acc: {round(test_acc,3)}\")\n",
    "    \n",
    "    sensi, speci, auc, f1_score = 0, 0, 0, 0\n",
    "    if report_on == 'train':\n",
    "        sensi, speci, auc, _ = report(train_label, train_data, classifier, features_selected_index)\n",
    "    if report_on == 'val':\n",
    "        sensi, speci, auc, _ = report(val_label, val_data, classifier, features_selected_index)\n",
    "    if report_on == 'test':\n",
    "        sensi, speci, auc, f1_score = report(test_label , test_data , classifier, features_selected_index, report_test=True)\n",
    "    return train_acc, val_acc, test_acc, sensi, speci, auc, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d90a05",
   "metadata": {},
   "source": [
    "# Define a function for final model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba2564cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_train (train_dataset, test_data, test_label, features_selected_index):\n",
    "    \"\"\"\n",
    "    Re-train with optimal hyperparameters and selected features to obtain the final model.\n",
    "    Report performance for the test set.\n",
    "    Parameters:\n",
    "        train_dataset: dictionary of the entire train set (i.e., the remaing of the original data set after hold-out the test set).\n",
    "        test_data: numpy array of radiomics features values of test set.\n",
    "        test_label: list of sample labels of test set\n",
    "    Returns:\n",
    "        test_acc: float, accuracy on the test set.\n",
    "        sensi: float, sensitivity on the test set.\n",
    "        speci: float, specificity on the test set.\n",
    "        auc: float, area under the Precicion-Recall curve.\n",
    "        f1_score: float, F1-score on the test set.\n",
    "    \"\"\"\n",
    "    train_ids = train_dataset.keys()    \n",
    "    train_data, train_label =  get_radiomics_label(train_ids, train_dataset)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_data)\n",
    "    train_data = scaler.transform(train_data)\n",
    "    test_data  = scaler.transform(test_data)\n",
    "    \n",
    "    train_data = train_data[:, features_selected_index]\n",
    "    test_data  = test_data [:, features_selected_index]\n",
    "    \n",
    "    classifier = SVC(probability=True, random_state=seed, C=C, gamma=gamma)\n",
    "    classifier.fit(train_data, train_label)\n",
    "    \n",
    "    test_acc  = classifier.score(test_data , test_label)\n",
    "    sensi, speci, auc, f1_score = report(test_label , test_data , classifier, features_selected_index, report_test=True)\n",
    "    return test_acc, sensi, speci, auc, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0639ec",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f4f30",
   "metadata": {},
   "source": [
    "## 1. Set inital parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18395864",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROI_path_19_20  = 'D:/Stone_composition/data_19_20/ROI/'  ### This dataset only contains 100UA patients\n",
    "dcm_path_19_20  = 'D:/Stone_composition/data_19_20/DICOM/'\n",
    "ROI_path_21  = 'D:/Stone_composition/data_21/ROI/' \n",
    "dcm_path_21  = 'D:/Stone_composition/data_21/DICOM/'\n",
    "label_file   = 'D:/Stone_composition/data_21/label.csv'\n",
    "params       = 'D:/Stone_composition/radiomics/binh/Params_stone_composition.yaml'\n",
    "feature_json = 'D:/Stone_composition/radiomics/data/11.json'\n",
    "modality     = 'CT_ax'\n",
    "\n",
    "number_of_folds = 5\n",
    "number_features = 20\n",
    "seed            = 2021\n",
    "alpha           = 0.08\n",
    "C               = 0.4\n",
    "gamma           = 0.075"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6964d7c",
   "metadata": {},
   "source": [
    "## 2. Create working data object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c67094d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data = create_data_dict ()\n",
    "# data2 = exclusion(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d62368",
   "metadata": {},
   "source": [
    "## 3. Train-test split data and 5-fold split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0eed6024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset, test_dataset = create_datasets(seed, data2) ### keys are sample_IDs\n",
    "train_dataset = utils.load_json(r\"E:\\UA_radiomics\\data_json\\11_train_sample.json\")\n",
    "test_dataset = utils.load_json(r\"E:\\UA_radiomics\\data_json\\11_test_sample.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72bc6946",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = test_dataset.keys()\n",
    "test_data, test_label =  get_radiomics_label(test_ids, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309d4b24",
   "metadata": {},
   "source": [
    "## 4. Extract and update the working data with radiomics features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d27cca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _, v1 in data2.items():\n",
    "#     for k, v in v1.items():\n",
    "#         if 'ax' in k:\n",
    "#             dcm_dir = v['dcm_dir']\n",
    "#             ROI_dir = v['ROI_dir']\n",
    "#             radiomics_raw = extract_one_sample (dcm_dir, ROI_dir, params)\n",
    "#             radiomics_filtered = {}\n",
    "#             for key, value in radiomics_raw.items():\n",
    "#                 if key[:5] in ['origi', 'wavel']: # only consider original and wavelet features\n",
    "#                     radiomics_filtered[key] = float(value)\n",
    "#             v['radiomics'] = radiomics_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fab6855",
   "metadata": {},
   "source": [
    "## 5. Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9afc2d0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: [20, 28, 214, 233, 420, 605, 544, 39, 323, 330, 46, 38, 173, 395, 209, 117, 157, 549, 140]\n",
      "original_firstorder_Maximum\n",
      "original_firstorder_Skewness\n",
      "wavelet-LHL_firstorder_Skewness\n",
      "wavelet-LHL_glcm_InverseVariance\n",
      "wavelet-HLL_glcm_JointAverage\n",
      "wavelet-HHL_glcm_InverseVariance\n",
      "wavelet-HLH_glszm_LowGrayLevelZoneEmphasis\n",
      "original_glcm_DifferenceEntropy\n",
      "wavelet-LHH_glcm_Idn\n",
      "wavelet-LHH_glcm_MCC\n",
      "original_glcm_Imc2\n",
      "original_glcm_DifferenceAverage\n",
      "wavelet-LLH_glszm_SizeZoneNonUniformity\n",
      "wavelet-HLL_firstorder_Median\n",
      "wavelet-LHL_firstorder_Median\n",
      "wavelet-LLH_firstorder_Minimum\n",
      "wavelet-LLH_glrlm_RunEntropy\n",
      "wavelet-HLH_glszm_SmallAreaLowGrayLevelEmphasis\n",
      "wavelet-LLH_glcm_InverseVariance\n"
     ]
    }
   ],
   "source": [
    "features_selected_index = most_crucial_fts (number_of_folds, number_features, train_dataset, modality, alpha)\n",
    "# features_selected_index = [20, 28, 214, 420, 605, 544, 323, 330, 233,  395]  ### best performance\n",
    "print (f\"Selected features: {features_selected_index}\")\n",
    "features_name = get_features_name()\n",
    "for i in features_selected_index:\n",
    "    print (features_name[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de96e5fa",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of fold #0\n",
      "Train_acc: 0.863; Val_acc: 0.94; Test_acc: 0.871\n",
      "Of fold #1\n",
      "Train_acc: 0.894; Val_acc: 0.819; Test_acc: 0.865\n",
      "Of fold #2\n",
      "Train_acc: 0.876; Val_acc: 0.902; Test_acc: 0.871\n",
      "Of fold #3\n",
      "Train_acc: 0.888; Val_acc: 0.854; Test_acc: 0.871\n",
      "Of fold #4\n",
      "Train_acc: 0.885; Val_acc: 0.902; Test_acc: 0.877\n",
      "Of fold #0\n",
      "Train_acc: 0.912; Val_acc: 0.94; Test_acc: 0.92\n",
      "Of fold #1\n",
      "Train_acc: 0.915; Val_acc: 0.892; Test_acc: 0.92\n",
      "Of fold #2\n",
      "Train_acc: 0.909; Val_acc: 0.939; Test_acc: 0.92\n",
      "Of fold #3\n",
      "Train_acc: 0.924; Val_acc: 0.89; Test_acc: 0.92\n",
      "Of fold #4\n",
      "Train_acc: 0.909; Val_acc: 0.927; Test_acc: 0.92\n",
      "Of fold #0\n",
      "Train_acc: 0.909; Val_acc: 0.952; Test_acc: 0.926\n",
      "Of fold #1\n",
      "Train_acc: 0.918; Val_acc: 0.88; Test_acc: 0.926\n",
      "Of fold #2\n",
      "Train_acc: 0.921; Val_acc: 0.902; Test_acc: 0.926\n",
      "Of fold #3\n",
      "Train_acc: 0.921; Val_acc: 0.878; Test_acc: 0.933\n",
      "Of fold #4\n",
      "Train_acc: 0.906; Val_acc: 0.939; Test_acc: 0.926\n",
      "Of fold #0\n",
      "Train_acc: 0.921; Val_acc: 0.94; Test_acc: 0.933\n",
      "Of fold #1\n",
      "Train_acc: 0.933; Val_acc: 0.892; Test_acc: 0.933\n",
      "Of fold #2\n",
      "Train_acc: 0.921; Val_acc: 0.939; Test_acc: 0.939\n",
      "Of fold #3\n",
      "Train_acc: 0.931; Val_acc: 0.878; Test_acc: 0.939\n",
      "Of fold #4\n",
      "Train_acc: 0.918; Val_acc: 0.951; Test_acc: 0.939\n",
      "Of fold #0\n",
      "Train_acc: 0.924; Val_acc: 0.929; Test_acc: 0.945\n",
      "Of fold #1\n",
      "Train_acc: 0.933; Val_acc: 0.892; Test_acc: 0.939\n",
      "Of fold #2\n",
      "Train_acc: 0.918; Val_acc: 0.939; Test_acc: 0.939\n",
      "Of fold #3\n",
      "Train_acc: 0.927; Val_acc: 0.902; Test_acc: 0.945\n",
      "Of fold #4\n",
      "Train_acc: 0.915; Val_acc: 0.951; Test_acc: 0.945\n",
      "Of fold #0\n",
      "Train_acc: 0.93; Val_acc: 0.94; Test_acc: 0.939\n",
      "Of fold #1\n",
      "Train_acc: 0.939; Val_acc: 0.88; Test_acc: 0.933\n",
      "Of fold #2\n",
      "Train_acc: 0.934; Val_acc: 0.939; Test_acc: 0.939\n",
      "Of fold #3\n",
      "Train_acc: 0.934; Val_acc: 0.915; Test_acc: 0.939\n",
      "Of fold #4\n",
      "Train_acc: 0.921; Val_acc: 0.951; Test_acc: 0.939\n",
      "Of fold #0\n",
      "Train_acc: 0.93; Val_acc: 0.952; Test_acc: 0.939\n",
      "Of fold #1\n",
      "Train_acc: 0.942; Val_acc: 0.916; Test_acc: 0.933\n",
      "Of fold #2\n",
      "Train_acc: 0.946; Val_acc: 0.939; Test_acc: 0.945\n",
      "Of fold #3\n",
      "Train_acc: 0.946; Val_acc: 0.915; Test_acc: 0.939\n",
      "Of fold #4\n",
      "Train_acc: 0.934; Val_acc: 0.951; Test_acc: 0.939\n",
      "Of fold #0\n",
      "Train_acc: 0.936; Val_acc: 0.94; Test_acc: 0.933\n",
      "Of fold #1\n",
      "Train_acc: 0.939; Val_acc: 0.916; Test_acc: 0.933\n",
      "Of fold #2\n",
      "Train_acc: 0.94; Val_acc: 0.939; Test_acc: 0.939\n",
      "Of fold #3\n",
      "Train_acc: 0.94; Val_acc: 0.915; Test_acc: 0.939\n",
      "Of fold #4\n",
      "Train_acc: 0.927; Val_acc: 0.963; Test_acc: 0.933\n",
      "Of fold #0\n",
      "Train_acc: 0.936; Val_acc: 0.929; Test_acc: 0.926\n",
      "Of fold #1\n",
      "Train_acc: 0.942; Val_acc: 0.916; Test_acc: 0.933\n",
      "Of fold #2\n",
      "Train_acc: 0.934; Val_acc: 0.951; Test_acc: 0.933\n",
      "Of fold #3\n",
      "Train_acc: 0.937; Val_acc: 0.902; Test_acc: 0.92\n",
      "Of fold #4\n",
      "Train_acc: 0.931; Val_acc: 0.951; Test_acc: 0.926\n",
      "Of fold #0\n",
      "Train_acc: 0.936; Val_acc: 0.929; Test_acc: 0.92\n",
      "Of fold #1\n",
      "Train_acc: 0.942; Val_acc: 0.904; Test_acc: 0.926\n",
      "Of fold #2\n",
      "Train_acc: 0.934; Val_acc: 0.951; Test_acc: 0.92\n",
      "Of fold #3\n",
      "Train_acc: 0.94; Val_acc: 0.902; Test_acc: 0.92\n",
      "Of fold #4\n",
      "Train_acc: 0.934; Val_acc: 0.939; Test_acc: 0.92\n",
      "Of fold #0\n",
      "Train_acc: 0.936; Val_acc: 0.929; Test_acc: 0.92\n",
      "Of fold #1\n",
      "Train_acc: 0.942; Val_acc: 0.904; Test_acc: 0.926\n",
      "Of fold #2\n",
      "Train_acc: 0.931; Val_acc: 0.951; Test_acc: 0.926\n",
      "Of fold #3\n",
      "Train_acc: 0.937; Val_acc: 0.902; Test_acc: 0.92\n",
      "Of fold #4\n",
      "Train_acc: 0.931; Val_acc: 0.939; Test_acc: 0.92\n",
      "Of fold #0\n",
      "Train_acc: 0.939; Val_acc: 0.929; Test_acc: 0.933\n",
      "Of fold #1\n",
      "Train_acc: 0.942; Val_acc: 0.916; Test_acc: 0.926\n",
      "Of fold #2\n",
      "Train_acc: 0.937; Val_acc: 0.951; Test_acc: 0.926\n",
      "Of fold #3\n",
      "Train_acc: 0.94; Val_acc: 0.902; Test_acc: 0.926\n",
      "Of fold #4\n",
      "Train_acc: 0.931; Val_acc: 0.939; Test_acc: 0.92\n",
      "Of fold #0\n",
      "Train_acc: 0.942; Val_acc: 0.929; Test_acc: 0.92\n",
      "Of fold #1\n",
      "Train_acc: 0.948; Val_acc: 0.916; Test_acc: 0.914\n",
      "Of fold #2\n",
      "Train_acc: 0.934; Val_acc: 0.963; Test_acc: 0.914\n",
      "Of fold #3\n",
      "Train_acc: 0.946; Val_acc: 0.902; Test_acc: 0.914\n",
      "Of fold #4\n",
      "Train_acc: 0.934; Val_acc: 0.951; Test_acc: 0.908\n",
      "Of fold #0\n",
      "Train_acc: 0.945; Val_acc: 0.94; Test_acc: 0.914\n",
      "Of fold #1\n",
      "Train_acc: 0.955; Val_acc: 0.916; Test_acc: 0.92\n",
      "Of fold #2\n",
      "Train_acc: 0.943; Val_acc: 0.963; Test_acc: 0.926\n",
      "Of fold #3\n",
      "Train_acc: 0.955; Val_acc: 0.902; Test_acc: 0.926\n",
      "Of fold #4\n",
      "Train_acc: 0.943; Val_acc: 0.951; Test_acc: 0.914\n",
      "Of fold #0\n",
      "Train_acc: 0.945; Val_acc: 0.952; Test_acc: 0.926\n",
      "Of fold #1\n",
      "Train_acc: 0.952; Val_acc: 0.916; Test_acc: 0.92\n",
      "Of fold #2\n",
      "Train_acc: 0.946; Val_acc: 0.963; Test_acc: 0.933\n",
      "Of fold #3\n",
      "Train_acc: 0.961; Val_acc: 0.902; Test_acc: 0.926\n",
      "Of fold #4\n",
      "Train_acc: 0.946; Val_acc: 0.951; Test_acc: 0.926\n",
      "Of fold #0\n",
      "Train_acc: 0.945; Val_acc: 0.929; Test_acc: 0.926\n",
      "Of fold #1\n",
      "Train_acc: 0.952; Val_acc: 0.916; Test_acc: 0.92\n",
      "Of fold #2\n",
      "Train_acc: 0.946; Val_acc: 0.963; Test_acc: 0.933\n",
      "Of fold #3\n",
      "Train_acc: 0.961; Val_acc: 0.902; Test_acc: 0.926\n",
      "Of fold #4\n",
      "Train_acc: 0.946; Val_acc: 0.951; Test_acc: 0.926\n",
      "Of fold #0\n",
      "Train_acc: 0.942; Val_acc: 0.94; Test_acc: 0.926\n",
      "Of fold #1\n",
      "Train_acc: 0.958; Val_acc: 0.916; Test_acc: 0.926\n",
      "Of fold #2\n",
      "Train_acc: 0.943; Val_acc: 0.963; Test_acc: 0.939\n",
      "Of fold #3\n",
      "Train_acc: 0.955; Val_acc: 0.902; Test_acc: 0.926\n",
      "Of fold #4\n",
      "Train_acc: 0.949; Val_acc: 0.951; Test_acc: 0.926\n",
      "Of fold #0\n",
      "Train_acc: 0.945; Val_acc: 0.94; Test_acc: 0.926\n",
      "Of fold #1\n",
      "Train_acc: 0.961; Val_acc: 0.916; Test_acc: 0.92\n",
      "Of fold #2\n",
      "Train_acc: 0.949; Val_acc: 0.963; Test_acc: 0.939\n",
      "Of fold #3\n",
      "Train_acc: 0.952; Val_acc: 0.902; Test_acc: 0.926\n",
      "Of fold #4\n",
      "Train_acc: 0.949; Val_acc: 0.951; Test_acc: 0.914\n",
      "Of fold #0\n",
      "Train_acc: 0.945; Val_acc: 0.929; Test_acc: 0.933\n",
      "Of fold #1\n",
      "Train_acc: 0.958; Val_acc: 0.916; Test_acc: 0.92\n",
      "Of fold #2\n",
      "Train_acc: 0.952; Val_acc: 0.963; Test_acc: 0.939\n",
      "Of fold #3\n",
      "Train_acc: 0.955; Val_acc: 0.89; Test_acc: 0.926\n",
      "Of fold #4\n",
      "Train_acc: 0.946; Val_acc: 0.951; Test_acc: 0.92\n"
     ]
    }
   ],
   "source": [
    "find_set_of_features = optimal_set_of_features (features_selected_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5187a1b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[20], 0.882, 0.946, 0.818, 0.9359999999999999],\n",
       " [[20, 28], 0.9179999999999999, 0.96, 0.874, 0.95],\n",
       " [[20, 28, 214], 0.9099999999999999, 0.95, 0.868, 0.958],\n",
       " [[20, 28, 214, 233], 0.9199999999999999, 0.95, 0.8880000000000001, 0.962],\n",
       " [[20, 28, 214, 233, 420],\n",
       "  0.9219999999999999,\n",
       "  0.9560000000000001,\n",
       "  0.8879999999999999,\n",
       "  0.966],\n",
       " [[20, 28, 214, 233, 420, 605], 0.924, 0.952, 0.898, 0.966],\n",
       " [[20, 28, 214, 233, 420, 605, 544],\n",
       "  0.9339999999999999,\n",
       "  0.968,\n",
       "  0.9039999999999999,\n",
       "  0.968],\n",
       " [[20, 28, 214, 233, 420, 605, 544, 39],\n",
       "  0.9339999999999999,\n",
       "  0.9620000000000001,\n",
       "  0.908,\n",
       "  0.9700000000000001],\n",
       " [[20, 28, 214, 233, 420, 605, 544, 39, 323],\n",
       "  0.9299999999999999,\n",
       "  0.962,\n",
       "  0.898,\n",
       "  0.9720000000000001],\n",
       " [[20, 28, 214, 233, 420, 605, 544, 39, 323, 330],\n",
       "  0.924,\n",
       "  0.952,\n",
       "  0.898,\n",
       "  0.9720000000000001],\n",
       " [[20, 28, 214, 233, 420, 605, 544, 39, 323, 330, 46],\n",
       "  0.924,\n",
       "  0.952,\n",
       "  0.898,\n",
       "  0.9720000000000001],\n",
       " [[20, 28, 214, 233, 420, 605, 544, 39, 323, 330, 46, 38],\n",
       "  0.9279999999999999,\n",
       "  0.952,\n",
       "  0.9019999999999999,\n",
       "  0.9720000000000001],\n",
       " [[20, 28, 214, 233, 420, 605, 544, 39, 323, 330, 46, 38, 173],\n",
       "  0.932,\n",
       "  0.952,\n",
       "  0.914,\n",
       "  0.9720000000000001],\n",
       " [[20, 28, 214, 233, 420, 605, 544, 39, 323, 330, 46, 38, 173, 395],\n",
       "  0.9339999999999999,\n",
       "  0.946,\n",
       "  0.924,\n",
       "  0.974],\n",
       " [[20, 28, 214, 233, 420, 605, 544, 39, 323, 330, 46, 38, 173, 395, 209],\n",
       "  0.9359999999999999,\n",
       "  0.952,\n",
       "  0.924,\n",
       "  0.97],\n",
       " [[20, 28, 214, 233, 420, 605, 544, 39, 323, 330, 46, 38, 173, 395, 209, 117],\n",
       "  0.932,\n",
       "  0.942,\n",
       "  0.924,\n",
       "  0.97],\n",
       " [[20,\n",
       "   28,\n",
       "   214,\n",
       "   233,\n",
       "   420,\n",
       "   605,\n",
       "   544,\n",
       "   39,\n",
       "   323,\n",
       "   330,\n",
       "   46,\n",
       "   38,\n",
       "   173,\n",
       "   395,\n",
       "   209,\n",
       "   117,\n",
       "   157],\n",
       "  0.9339999999999999,\n",
       "  0.946,\n",
       "  0.924,\n",
       "  0.9720000000000001],\n",
       " [[20,\n",
       "   28,\n",
       "   214,\n",
       "   233,\n",
       "   420,\n",
       "   605,\n",
       "   544,\n",
       "   39,\n",
       "   323,\n",
       "   330,\n",
       "   46,\n",
       "   38,\n",
       "   173,\n",
       "   395,\n",
       "   209,\n",
       "   117,\n",
       "   157,\n",
       "   549],\n",
       "  0.9339999999999999,\n",
       "  0.946,\n",
       "  0.924,\n",
       "  0.9720000000000001],\n",
       " [[20,\n",
       "   28,\n",
       "   214,\n",
       "   233,\n",
       "   420,\n",
       "   605,\n",
       "   544,\n",
       "   39,\n",
       "   323,\n",
       "   330,\n",
       "   46,\n",
       "   38,\n",
       "   173,\n",
       "   395,\n",
       "   209,\n",
       "   117,\n",
       "   157,\n",
       "   549,\n",
       "   140],\n",
       "  0.93,\n",
       "  0.946,\n",
       "  0.914,\n",
       "  0.9719999999999999]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_set_of_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e965fc9f",
   "metadata": {},
   "source": [
    "## 6. Train and evaluate by 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc0277b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of fold #0\n",
      "Train_acc: 0.945; Val_acc: 0.929; Test_acc: 0.933\n",
      "Of fold #1\n",
      "Train_acc: 0.958; Val_acc: 0.916; Test_acc: 0.92\n",
      "Of fold #2\n",
      "Train_acc: 0.952; Val_acc: 0.963; Test_acc: 0.939\n",
      "Of fold #3\n",
      "Train_acc: 0.955; Val_acc: 0.89; Test_acc: 0.926\n",
      "Of fold #4\n",
      "Train_acc: 0.946; Val_acc: 0.951; Test_acc: 0.92\n",
      "Mean train_acc: 0.95, val_acc: 0.93, val_sen: 0.95, val_spe: 0.91, val_AUC: 0.97\n"
     ]
    }
   ],
   "source": [
    "train, val, sen, spe, AUPRC = 0, 0, 0, 0, 0\n",
    "for fold in range (number_of_folds):\n",
    "    train_acc, val_acc, test_acc, sensi, speci, auprc, f1_score = training (fold, test_data, test_label, features_selected_index, report_on='val')\n",
    "    train += train_acc\n",
    "    val   += val_acc\n",
    "    sen   += sensi\n",
    "    spe   += speci\n",
    "    AUPRC += auprc\n",
    "print (f\"Mean train_acc: {round((train/5),2)}, val_acc: {round((val/5),2)}, val_sen: {round((sen/5),2)}, val_spe: {round((spe/5),2)}, val_AUC: {round((AUPRC/5),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0b122b",
   "metadata": {},
   "source": [
    "## 7. Performance of the final model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e30120cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1=0.874 AUPRC=0.955\n"
     ]
    }
   ],
   "source": [
    "test_acc, sensi, speci, auc, f1_score = re_train (train_dataset, test_data, test_label, features_selected_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
